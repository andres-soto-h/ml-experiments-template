{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "involved-declaration",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "In this notebook we report progress on the project of house price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aerial-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "confidential-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legal-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "successful-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inclusive-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(*, model, metric, X_train, y_train, X_test, y_test):\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    train_error = metric(y_train, train_predictions)\n",
    "    test_error = metric(y_test, test_predictions)\n",
    "    return {\n",
    "        \"train_predictions\": train_predictions,\n",
    "        \"test_predictions\": test_predictions,\n",
    "        \"train_error\": train_error,\n",
    "        \"test_error\": test_error\n",
    "    }\n",
    "\n",
    "def print_report(*, model, evaluation):\n",
    "    print(f\"Model used:\\n\\t{reg}\")\n",
    "    print(f\"Error:\\n\\ttrain set {evaluation['train_error']}\\n\\tTest error: {evaluation['test_error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hazardous-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "union-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "central-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.get_dataset(\n",
    "    partial(pd.read_csv, filepath_or_buffer=dataset_path),\n",
    "    splits=(\"train\", \"test\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-pastor",
   "metadata": {},
   "source": [
    "**If you need to visualize anything from your training data, do it here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-speed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-freight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "specific-solid",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Before doing any complex Machine Learning model, let's try to solve the problem by having an initial educated guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blank-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('average-price-per-neighborhood-regressor',\n",
      "                 AveragePricePerNeighborhoodRegressor())])\n",
      "Error:\n",
      "\ttrain set 33609.9990756996\n",
      "\tTest error: 34799.09487677742\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-04 15-12\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-input",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "indian-senate",
   "metadata": {},
   "source": [
    "## Linear Regression Model \n",
    "\n",
    "We want to try easy things first, so know lets see how a linear regression model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "defined-publicity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('age-extractor', AgeExtractor()),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('linear-regressor', LinearRegression())])\n",
      "Error:\n",
      "\ttrain set 11261.923628487202\n",
      "\tTest error: 25474507852808.133\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-04 15-13\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-marriage",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort and discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-description",
   "metadata": {},
   "source": [
    "## Linear regression with Feature Engineering\n",
    "\n",
    "Probably the previous model is not good enough, let's see how is the performance of a model using some produced features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "report = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-integral",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-merchandise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opened-borough",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "Let's assume you are overfitting. Load the results of a linear regression model with regularized loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "report = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-batman",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-science",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hidden-begin",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Decision trees ofer great complexity, they can fit even a noisy dataset almost perfectly. Let's see how it behaves on the task at hand. \n",
    "\n",
    "**Overfiting case**\n",
    "Let's see the results for a model that has greatly overfit the data, this wouldn't be an ideal model, but at least it could tell that our model is powerful enough for the task at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "report = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-identification",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-leader",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blessed-pattern",
   "metadata": {},
   "source": [
    "**Using best hyper params** Now let's see thow much a simple decision tree can give us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "report = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-order",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "assigned-acting",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Now it is time to use a model that can properly help us to regularize the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-situation",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-profession",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-unemployment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
